# Evaluating Przeprogramowani Website

Systematically evaluate website implementations against established benchmark criteria.

Evaluation consists of two parts:
1) Automatic Agent Evaluation (launch dev server and evaluate the implementation)
2) Human-provided Feedback (AskUserQuestion tool based on criteria)

## Quick start

Run evaluation on provided directory (ask user for the path to the directory if not provided).

## Output

Generates `eval-results.csv` in `./eval-results/{model-name}-attempt-{number}/eval-results.csv` directory

There's not other output files generated by this skill.

## Criteria Assignment by Phase

| Phase | Criterion | Type | Method |
|-------|-----------|------|--------|
| 1 | Local build | Auto | Check build success |
| 2 | Manual testing | User | AskUserQuestion |
| 2 | Consistent UI | User | AskUserQuestion |
| 2 | Responsive design | User | AskUserQuestion |
| 3 | Tech stack | Auto | Agent evaluation |
| 3 | "O nas" page | Auto | Agent evaluation |
| 3 | "Podcast" page | Auto | Agent evaluation |
| 3 | "YouTube" page | Auto | Agent evaluation |
| 3 | "Kursy" section | Auto | Agent evaluation |
| 3 | SEO Tags | Auto | Agent evaluation |

## Evaluation workflow

### ⚠️ Critical: Build Failure Rule
**If the build fails OR the dev server cannot start, the entire evaluation is halted:**
- Local build criterion: **0/1** (build failed)
- All other 9 criteria: **0/1** (cannot be tested without running build)
- Task completion time: **N/A**

This is a hard stop - no manual testing, no automatic evaluation. Generate results immediately with all zeros and brief notes explaining that evaluation was impossible due to build/server failure.

### Phase 1: Build & Server Setup
1. Ask user for the path to the directory if not provided
2. Read evaluation criteria from `_benchmark/criteria.md` file
3. Navigate to provided directory and execute:
   - `npm install` - Install dependencies
   - `npm run build` - Build the project (Agent evaluates: Local build criterion)
   - **IF BUILD FAILS**: Stop immediately, generate CSV with all zeros, halt evaluation
   - `npm run dev` - Start development server
   - **IF DEV SERVER FAILS TO START**: Stop immediately, generate CSV with all zeros, halt evaluation
4. Extract dev server URL (e.g., `http://localhost:3000`) and display it to the user

### Phase 2: Manual User Evaluation (3 criteria via AskUserQuestion)
5. Ask user to open the dev server URL and evaluate these 3 criteria:
   - **Overall runtime quality** → "Manual testing" criterion (line 64-74 in criteria.md)
     - Is the local dev server running and all pages accessible?
     - Do all pages return 200 status code?
     - Are navigation, styles, and content working properly?
   - **UI Consistency** → "Consistent UI" criterion (line 153-162 in criteria.md)
     - Is the UI consistent across all pages?
     - Are there any broken or inconsistent styles?
   - **Responsive design** → "Responsive design" criterion (line 164-173 in criteria.md)
     - Are containers, components and navigation responsive on different screen sizes?
     - Are there any layout or responsiveness issues?

6. Use `AskUserQuestion` tool to collect user feedback for each of the 3 criteria
   - For each criterion: ask user to provide a score (1, 0.5, or 0) and notes

### Phase 3: Automatic Agent Evaluation (7 criteria)
7. Evaluate remaining 7 criteria automatically:
   - **Tech stack** - Verify exact dependencies (Astro 5, React 19, Tailwind CSS 4, Cloudflare Adapter)
   - **"O nas" page** - Verify content matches context file and returns 200
   - **"Podcast" page** - Verify both podcasts displayed with real data (not mocks)
   - **"YouTube" page** - Verify real Przeprogramowani channel videos (not mocks/placeholders)
   - **"Kursy" section** - Verify content matches context and 10xDevs featured on main page
   - **SEO Tags** - Verify SEO component exists and content matches context
   - (Note: "Local build" was already evaluated in Phase 1)

### Phase 4: Generate Results
8. Generates `eval-results.csv` in `./eval-results/{model-name}-attempt-{number}/eval-results.csv` directory:
   - Include all 10 criteria rows with scores and notes
   - Use placeholder TIME_TAKEN for task completion time - provided by the user in Phase 2, skipped during agent evaluation
   - For content verification criteria, always note specific discrepancies in the notes field

## Critical evaluation notes - Content verification

### Podcast Page (0.5 pts trap)
⚠️ **Common hallucination**: Models often generate realistic-looking episode lists with fake titles, dates, and descriptions. Verify:
- Extract actual Spotify/Apple Podcasts URLs from source code
- Compare to context file URLs - must match exactly (show IDs differ between podcasts)
- Sample episodes: Are they real or fabricated? (e.g., made-up episode titles/dates)
- Even if "real-looking" data exists in source code files, it may not be actual podcast content

**Action**: Always verify podcast URLs and episode data against `./context/przeprogramowani.md` file. Deduct 0.5 pts if URLs are wrong or episode data appears generated/hallucinated.

### YouTube Page (0.5 pts trap)
⚠️ **Rick roll detection**: Pages may display correct UI with valid-looking video IDs that actually point to wrong content (e.g., dQw4w9WgXcQ = Rick Roll). Verify:
- Check video data source (e.g., `src/data/videos.ts`)
- Look for repeated/placeholder YouTube IDs (dQw4w9WgXcQ, xdXblEEP0MQ, etc.)
- Verify video titles and descriptions match actual Przeprogramowani channel content
- Check if same ID is used for multiple videos (hallucination indicator)

**Action**: Video IDs should be unique and match actual Przeprogramowani channel videos. Deduct 0.5 pts if using placeholder/rick roll IDs or if content doesn't match channel.

### "O nas" Page (0.5 pts trap)
⚠️ **Team member descriptions**: Simplified bios or paraphrased descriptions may not match context file. Verify:
- Team member names match: Przemek Smyrdek, Marcin Czarkowski
- Key details present: DAZN/Cabify for Przemek, SmartRecruiters for Marcin, their course assignments
- Descriptions should capture their professional background and contributions

**Action**: Deduct 0.5 pts if team descriptions are simplified, missing key details, or don't match context.

### External Links & URLs
⚠️ **Link accuracy is critical**: Verify:
- Podcast Spotify URLs in data files match context file (different show IDs for each podcast)
- Course URLs point to correct domains (10xdevs.pl, opanujfrontend.pl, etc.)
- Social media links are valid (LinkedIn, Twitter, GitHub)

**Action**: Always extract and compare URLs against context file. URLs are objective - if they don't match, it's a clear discrepancy.

## Complete Content Verification Checklist

### 1. External URLs Verification
- [ ] Extract Spotify URLs from source code (e.g., `src/data/podcasts.ts`)
- [ ] Compare against context file - must match exactly
  - Przeprogramowani: `3yVvOAXSYq6sQB02w4A4wo`
  - Opanuj.AI: `3D6LmchBdoqL2sWkQjvWOy`
- [ ] Check course URLs point to correct domains
- [ ] Verify social media links (LinkedIn, Twitter, GitHub)

### 2. Team Member Bios
- [ ] Names match context: Przemek Smyrdek, Marcin Czarkowski
- [ ] Key professional details present:
  - Przemek: DAZN, Cabify experience mentioned
  - Marcin: SmartRecruiters experience mentioned
- [ ] Course assignments listed correctly
- [ ] Not oversimplified or paraphrased

### 3. Episode/Video Data Red Flags
- [ ] No repeated IDs across multiple items (each video has unique YouTube ID)
- [ ] Dates format and regularity seem natural (not artificially regular)
- [ ] Descriptions match actual content (not generic placeholders)
- [ ] Check for placeholder/rick roll IDs:
  - `dQw4w9WgXcQ` = Rick Roll
  - `xdXblEEP0MQ` = Placeholder
  - These should NOT appear in real implementations

### 4. Podcast Episodes
- [ ] Both podcasts displayed (Przeprogramowani & Opanuj.AI)
- [ ] Episode data comes from actual data files (not just mocks)
- [ ] Episode titles sound realistic and specific (not generic)
- [ ] Dates are consistent with real podcast schedules
- [ ] Episode URLs are valid

## Scoring Guidance

- **1.0 point**: Content verified against actual external sources and context file, all data matches
- **0.5 points**: Content structure is correct but links/URLs are inaccurate or episode/video data appears hallucinated/mocked
- **0 points**: Page missing, doesn't load, or major structural issues

**Always note specific discrepancies in CSV notes field for transparency.**